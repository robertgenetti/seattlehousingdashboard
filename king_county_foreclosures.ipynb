{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Seattle, WA) Distressed Property Dashboard Project\n",
    "## Python, SQL and Tableau: Data Extraction, Key Metrics, Dashboard\n",
    "- Data Sources: King County Records, Assessor Parcel Website\n",
    "- Dataset: Custom\n",
    "- Export Option: SQL database\n",
    "- Data Transformations: Concat multiple fields, remove unknown columns\n",
    "- Future changes: Add historical trend column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7v9X5XGjjJyU"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from re import findall, sub\n",
    "from datetime import date, timedelta, datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from tqdm import notebook\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "T214B60j8p2r"
   },
   "outputs": [],
   "source": [
    "# Set current directory\n",
    "os.chdir('/home/jovyan/work')\n",
    "\n",
    "# Setup chromedriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Set constants\n",
    "records_url = \"https://recordsearch.kingcounty.gov/LandmarkWeb/search/index?theme=.blue&section=searchCriteriaDocuments&quickSearchSelection=\"\n",
    "pid_url = 'https://blue.kingcounty.com/Assessor/eRealProperty/Detail.aspx?ParcelNbr='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "h-e0aE7mOf8e"
   },
   "outputs": [],
   "source": [
    "# Get driver\n",
    "def driver_open(url=records_url):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=options)\n",
    "    driver.get(url)\n",
    "    driver.set_window_size(1200,850)\n",
    "    return driver\n",
    "\n",
    "# Get record for given time period\n",
    "def get_records_from_dates(beg,end,n_iter,driver):\n",
    "    if n_iter == 0:\n",
    "        li_doctype = driver.find_element('id','searchCriteriaDocuments-tab')\n",
    "        li_doctype.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        text_area = driver.find_element('id','documentType-DocumentType')\n",
    "        text_area.send_keys(\"NTS\")\n",
    "\n",
    "    b_date = driver.find_element('id','beginDate-DocumentType')\n",
    "    b_date.clear()\n",
    "    b_date.send_keys(beg)\n",
    "\n",
    "    e_date = driver.find_element('id','endDate-DocumentType')\n",
    "    e_date.clear()\n",
    "    e_date.send_keys(end)\n",
    "\n",
    "    submit = driver.find_element('id','submit-DocumentType')\n",
    "    submit.click()\n",
    "    time.sleep(3)\n",
    "    if n_iter == 0:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.save_screenshot(f'tmp/screenshot.png')\n",
    "\n",
    "    df = pd.read_html(driver.page_source,skiprows=0)[-2]\n",
    "    df = df.drop(columns=['#','Unnamed: 1', 'Unnamed: 2', \n",
    "                        'Unnamed: 4', 'Book', 'Book Type',\n",
    "                        'DocLinks.1','Unnamed: 15', 'Page'])\n",
    "    return df\n",
    "  \n",
    "# Get complete dataframe\n",
    "def fetch_records(n_weeks):\n",
    "    date_format = \"%m/%d/%Y\"\n",
    "    x = datetime.today()\n",
    "    df = pd.DataFrame()\n",
    "    dates = []\n",
    "\n",
    "    for i in range(n_weeks):\n",
    "        x = x - timedelta(days=1)\n",
    "        end = x - timedelta(weeks=1)\n",
    "        dates.append(x.strftime(date_format)) \n",
    "        dates.append(end.strftime(date_format)) \n",
    "        x = end\n",
    "    dates_reversed = dates[::-1]\n",
    "    print('Looking up records for the week(s) of',', '.join(dates_reversed[::2]))\n",
    "    driver = driver_open()\n",
    "    n_dates = int(len(dates)/2)\n",
    "    \n",
    "    print('Fetching records now')\n",
    "    for i in notebook.tqdm(range(n_dates), total=n_dates):\n",
    "        file_name = \"{:02}\".format(i)\n",
    "        partial = get_records_from_dates(dates[i*2+1],dates[i*2], i, driver)\n",
    "        df = pd.concat([df,partial], ignore_index=True, sort=False)\n",
    "\n",
    "    driver.quit()\n",
    "    return df\n",
    "\n",
    "# Get Address and Appraised Value\n",
    "def add_details_cols(pids):\n",
    "    address_list = []\n",
    "    appraised_list = []\n",
    "\n",
    "    print('Adding parcel info')\n",
    "    for pid in notebook.tqdm(pids, total=len(pids)):\n",
    "        url = pid_url + pid\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        try:\n",
    "            address_cell = soup.find('table',id='cphContent_DetailsViewParcel').find_all('td')[5]\n",
    "            address_list.append(address_cell.text)\n",
    "            appraise_cell = soup.find('table',id='cphContent_GridViewTaxRoll').find_all('td')[7]\n",
    "            appraised_list.append(appraise_cell.text)\n",
    "        except:\n",
    "            address_list.append(None)\n",
    "            appraised_list.append(None)\n",
    "    \n",
    "        time.sleep(1)\n",
    "    return pd.Series(address_list), pd.Series(appraised_list)\n",
    "\n",
    "# Get Location\n",
    "def add_location_cols(addresses):\n",
    "    lat = []\n",
    "    lon = []\n",
    "    hood = []\n",
    "    metro = []\n",
    "    geolocator = Nominatim(user_agent=\"KingCounty\")\n",
    "\n",
    "    print('Adding location info')\n",
    "    try:  \n",
    "        for address in notebook.tqdm(addresses, total=len(addresses)):\n",
    "            try:\n",
    "                location = geolocator.geocode(str(address) + ', WA')\n",
    "            except:\n",
    "                location = None\n",
    "            if location and location.raw and (len(location.raw['display_name'].split(', ')) > 2):\n",
    "                lat.append(location.latitude)\n",
    "                lon.append(location.longitude)\n",
    "                address = location.raw['display_name'].split(', ')\n",
    "                hood.append(address[2])\n",
    "                metro.append(address[3])\n",
    "\n",
    "            else:\n",
    "                lat.append(None)\n",
    "                lon.append(None)\n",
    "                hood.append(None)\n",
    "                metro.append(None)\n",
    "\n",
    "        time.sleep(1.5)\n",
    "    except ValueError as e:\n",
    "        print('An error occurred: {}\\n{}'.format(str(e),location))\n",
    "\n",
    "    return pd.Series(lat), pd.Series(lon) , pd.Series(hood), pd.Series(metro)\n",
    "\n",
    "# Add PID, Address, Apprased Value to dataframe\n",
    "def transform(df):\n",
    "    # Apply hashing function to the column\n",
    "    df['Grantor'] = df['Grantor'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "    df['PID'] = df['Legal'].apply(lambda x: str(x.strip().split(\" \")[1]) if x.find('PID') != -1 else '')\n",
    "    df.drop(columns=['Legal'], inplace=True)\n",
    "    df['Address'], df['Appraised Value'] = add_details_cols(list(df['PID']))\n",
    "    df['Latitude'], df['Longitude'], df['Neighborhood'], df['Metro'] = add_location_cols(df['Address'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# check if local file is older than 1 week\n",
    "def isWeek_old(filename):\n",
    "    epoch = os.path.getmtime(f'data/{filename}.csv')\n",
    "    org = date.fromtimestamp(epoch)\n",
    "    is_old = org + timedelta(weeks=1) < date.today()\n",
    "    return is_old\n",
    "\n",
    "#  Load Date\n",
    "def load(filename, n_weeks=1):\n",
    "    try:\n",
    "        df = fetch_records(n_weeks)\n",
    "        df = transform(df)\n",
    "        if df.empty:\n",
    "            raise Exception(\"Dataframe appears to be empty, please try again later...\")\n",
    "        \n",
    "        org_rows = df.shape[0]\n",
    "        df.drop_duplicates(subset=['Address'], inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        print(f'{org_rows - df.shape[0]} were deleted')\n",
    "        \n",
    "        df.to_csv(f'data/{filename}.csv') \n",
    "        print(f'Save to file: data/{filename}.csv')\n",
    "\n",
    "        return df\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "\n",
    "# Cache dataset to local disk or reload and save if older than 1 week\n",
    "def refresh_data(filename):\n",
    "    # init data fetch first time\n",
    "    if f'{filename}.csv' not in os.listdir('data'):\n",
    "        df = load(filename, n_weeks=12)\n",
    "    \n",
    "    # refresh data if old\n",
    "    elif isWeek_old(filename):\n",
    "        date_format = \"%m/%d/%Y\"\n",
    "        df = pd.read_csv(f'data/{filename}.csv',index_col='Unnamed: 0')\n",
    "        last_update = df['Record Date'].max()\n",
    "        start = datetime.strptime(last_update,date_format)\n",
    "        x = datetime.today()\n",
    "        diff = x - start\n",
    "        n_weeks = math.ceil(diff.days/7)\n",
    "        df = load(filename, n_weeks=n_weeks)\n",
    "        \n",
    "    else:\n",
    "        df = pd.read_csv(f'data/{filename}.csv',index_col='Unnamed: 0')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "FdtLRnILpa96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up records for the week(s) of 06/20/2022, 06/28/2022, 07/06/2022, 07/14/2022, 07/22/2022, 07/30/2022, 08/07/2022, 08/15/2022, 08/23/2022, 08/31/2022, 09/08/2022, 09/16/2022\n",
      "Fetching records now\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a2d35be4e467b999562461be9fee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding parcel info\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4245eae5c2fb4a2c9006cf6c04045ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding location info\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371594ebece4f2496986f5db6efffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 were deleted\n",
      "Save to file: data/seattle-distressed-properties.csv\n"
     ]
    }
   ],
   "source": [
    "df = refresh_data('seattle-distressed-properties')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 134 entries, 0 to 169\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Status           134 non-null    object \n",
      " 1   Grantor          134 non-null    object \n",
      " 2   Grantee          134 non-null    object \n",
      " 3   Record Date      134 non-null    object \n",
      " 4   Doc Type         134 non-null    object \n",
      " 5   Rec. #           134 non-null    int64  \n",
      " 6   DocLinks         134 non-null    object \n",
      " 7   PID              134 non-null    object \n",
      " 8   Address          134 non-null    object \n",
      " 9   Appraised Value  134 non-null    object \n",
      " 10  Latitude         134 non-null    float64\n",
      " 11  Longitude        134 non-null    float64\n",
      " 12  Neighborhood     134 non-null    object \n",
      " 13  Metro            134 non-null    object \n",
      "dtypes: float64(2), int64(1), object(11)\n",
      "memory usage: 15.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Address'].isna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:headband@192.168.0.151:5432/mydb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note to self: inlcude a unique index to use for postgres database insert functionality\n",
    "\n",
    "CREATE UNIQUE INDEX CONCURRENTLY seattle_rec_num \n",
    "ON SeattleDistressedProp (\"Rec. #\");\n",
    "\n",
    "ALTER TABLE SeattleDistressedProp \n",
    "ADD CONSTRAINT unique_rec_num \n",
    "UNIQUE USING INDEX seattle_rec_num;\n",
    "\"\"\"\n",
    "\n",
    "def update_server(table_name):\n",
    "    table_exists = (\n",
    "    'SELECT EXISTS (SELECT FROM pg_tables '\n",
    "    'WHERE schemaname = \\'public\\' AND '\n",
    "    f'tablename  = \\'{table_name}\\');'\n",
    "    )\n",
    "    \n",
    "    row_count = (\n",
    "    'SELECT count(*) '\n",
    "    f'FROM \"{table_name}\";'\n",
    "    )\n",
    "  \n",
    "    try:\n",
    "        if engine.execute(table_exists).first()[0]:\n",
    "            org_rows = engine.execute(row_count).first()[0]\n",
    "            cols = \",\".join(['\"'+i+'\"' for i in list(df.columns)])\n",
    "            query = (\n",
    "            f'INSERT INTO \"{table_name}\"({cols}) '\n",
    "            f'VALUES {\",\".join([str(i) for i in list(df.to_records(index=False))])} '\n",
    "            f'ON CONFLICT (\"Rec. #\") DO NOTHING;'\n",
    "            )\n",
    "            print(query)\n",
    "            engine.execute(query)\n",
    "            new_rows = engine.execute(row_count).first()[0]\n",
    "            diff_rows = new_rows - org_rows\n",
    "            print(diff_rows,' rows added')\n",
    "        else:\n",
    "            df.to_sql(table_name,con=engine,index=False,if_exists='replace')\n",
    "        print('Successfully updated server')\n",
    "    except Exception as err:\n",
    "        print('Got an error while updating server')\n",
    "        print('Error: ', str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated server\n"
     ]
    }
   ],
   "source": [
    "update_server(\"SeattleDistressedProp\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
